# Question Pro - Seq2Seq

## Introduction
The Seq2Seq model in QuestionPro utilizes a robust sequence-to-sequence architecture to generate questions from provided contexts. This model, rooted in deep learning and natural language processing, excels at understanding and transforming input sequences into meaningful output sequences.

With Seq2Seq, the task of question generation becomes dynamic and context-aware. The model can capture intricate relationships within a context and translate them into well-structured questions. By leveraging the inherent sequential nature of language, Seq2Seq enhances the precision and relevance of the generated questions.

## Dataset and Model
- Link for dataset and model: [Dropbox](https://www.dropbox.com/scl/fo/06z72prw84qvdon24zve7/h?rlkey=92nr17ygw0ghhuies0qwgavvp&dl=0)
- Stanford Question Answering Dataset [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/)

## Requirements
- Language: Python3.8
- Packages
  -  torch: 1.5.0
  -  spacy: 2.2.4
  -  torchtext: 0.3.1

## Preprocessing Steps

- Case Normalization: Convert all text to lowercase or uppercase to ensure uniformity.
- Tokenization: Break text into individual tokens (words or subword units) to facilitate processing.
- Named Entity Recognition (NER): Identify and classify entities (e.g., names, locations) within the text.
- POS-Tagging (Part-of-Speech Tagging): Assign grammatical parts of speech to each token in the text.
- IOB-Tagging (Inside-Outside-Beginning Tagging): Label tokens with Inside, Outside, or Beginning tags to represent entities sequentially.
- Pairing Input and Output: Organize the preprocessed data into input-output pairs for training the Seq2Seq model.

These preprocessing steps lay the foundation for training a powerful Seq2Seq model by ensuring the input data is appropriately formatted and enriched with linguistic information. Each step is crucial in preparing the data for effective sequence-to-sequence processing.

## Model Architecture
# Seq2Seq Model Architecture
The Seq2Seq (Sequence-to-Sequence) model employed in this project is designed to convert input sequences into output sequences, making it particularly effective for tasks like question generation. Below is an overview of the key components of the Seq2Seq model architecture.

### Encoder

The encoder is responsible for processing the input sequence and capturing its contextual information. It converts the input sequence into a fixed-size context vector, the foundation for generating the output sequence.

### Decoder

The decoder takes the context vector generated by the encoder and uses it to produce the output sequence. It does so step-by-step, generating one token at a time while considering the previously generated tokens. This process continues until the entire output sequence is produced.

### Attention Mechanism

An attention mechanism is employed to enhance the model's ability to capture relevant information from the input sequence during decoding. This allows the model to focus on different parts of the input sequence at different decoding steps, improving its overall performance.

### Training

The Seq2Seq model is trained using pairs of input and target sequences. The training involves optimizing the model's parameters to minimize the difference between the predicted output and the target sequence. This is typically done using techniques like teacher forcing.

### Hyperparameters

Key hyperparameters, such as the learning rate, batch size, and model architecture specifics, can be configured to fine-tune the performance of the Seq2Seq model. Adjusting these parameters may be necessary based on the dataset's characteristics and the task's nature.

This model architecture provides a robust framework for sequence-to-sequence tasks, offering flexibility and effectiveness in various natural language processing applications.

## API test

```
/seq2seq
{
  "context": " ",
  "answer": " ",
  "answer_start": [int]
  }
```
