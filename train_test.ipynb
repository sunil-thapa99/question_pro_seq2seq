{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UeKQWgVHmN6-"
      },
      "outputs": [],
      "source": [
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py38_4.8.2-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py38\" --user"
      ],
      "id": "UeKQWgVHmN6-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qW51ARkmRnN",
        "outputId": "a8fc04e1-c549-4408-c83e-39b242ceabb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Current Version:- 3.8.18 (default, Sep 11 2023, 13:20:55) \n",
            "[GCC 11.2.0]\n"
          ]
        }
      ],
      "source": [
        "# Reload the web page and execute this cell\n",
        "import sys\n",
        "print(\"User Current Version:-\", sys.version)"
      ],
      "id": "0qW51ARkmRnN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fed646e3",
        "outputId": "f551a0fe-f739-43c7-89c3-8c423ee90735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.8.18\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ],
      "id": "fed646e3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqZmc20jZ-Y-",
        "outputId": "a7db5f22-ebfb-4505-e4a0-3275941d31db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "hqZmc20jZ-Y-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpBy7ljrZ3jX",
        "outputId": "6494ea58-8c1c-4957-8950-10c732ddbb03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/automatic-question-generation-master/'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/automatic-question-generation-master/"
      ],
      "id": "EpBy7ljrZ3jX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnc4Zo8obhAg",
        "outputId": "8036c4fa-1008-4036-c055-b559ce3ae11f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "download_squad.py  evaluate.py  models.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  README.md  \u001b[01;34mruns\u001b[0m/  squad_parse.py  train.py\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ],
      "id": "jnc4Zo8obhAg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9nj-1kZbkeo"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ],
      "id": "R9nj-1kZbkeo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75bd2e34"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "DIR = os.getcwd()"
      ],
      "id": "75bd2e34"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd18a801"
      },
      "outputs": [],
      "source": [
        "!python main/download_squad.py --directory dataset"
      ],
      "id": "dd18a801"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "953bac96",
        "outputId": "04a99cfd-e805-48e0-bbb8-484813e5498f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing training...\n",
            "\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python ../main/squad_parse.py --train_filepath ../dataset/train-v2.0.json --dev_filepath ../dataset/dev-v2.0.json"
      ],
      "id": "953bac96"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c565c45",
        "outputId": "330eafa2-520f-4549-eb1d-227a5a426646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/automatic-question-generation-master/main\n"
          ]
        }
      ],
      "source": [
        "%cd main"
      ],
      "id": "2c565c45"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBIxiXphMABS"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.5 spacy==2.2.4 torchtext==0.3.1"
      ],
      "id": "RBIxiXphMABS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A8g84_JMP-v",
        "outputId": "32fb058c-2007-4dae-8c76-9b2d0aa80910"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch                     1.5.0              \n",
            "torchtext                 0.3.1              \n"
          ]
        }
      ],
      "source": [
        "!pip list | grep torch"
      ],
      "id": "0A8g84_JMP-v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GgjcK3HeQpt"
      },
      "outputs": [],
      "source": [
        "!pip install -U numpy"
      ],
      "id": "4GgjcK3HeQpt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7be6efb4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import data\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "7be6efb4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a692bdb7"
      },
      "outputs": [],
      "source": [
        "train_set_path = os.path.join(DIR, 'results/resultssquad_train.csv')\n",
        "dev_set_path = os.path.join(DIR, 'results/resultssquad_dev.csv')\n",
        "test_size = 0.7\n",
        "save = os.path.join(DIR, 'dataset')\n",
        "train_set = os.path.join(DIR, 'dataset')\n",
        "word_vector = 'glove'\n",
        "batch_size = 128\n",
        "numberbatch_loc = os.path.join(DIR, 'dataset')\n",
        "resume = ''\n",
        "epochs = 10"
      ],
      "id": "a692bdb7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7095e4bf"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "id": "7095e4bf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN2-SKkzhsVG",
        "outputId": "3abb0954-7a05-4dbc-f644-e5e20a64e2b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ],
      "id": "HN2-SKkzhsVG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c49acf7a"
      },
      "outputs": [],
      "source": [
        "# Split dev dataset into test set and validation set\n",
        "dev_set = pd.read_csv(dev_set_path)\n",
        "validation_set, test_set = train_test_split(dev_set, test_size = test_size)"
      ],
      "id": "c49acf7a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paAsSPvgaeUF",
        "outputId": "28d41aa6-e0a5-4be3-b161-2cd168aef0f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "130319"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "original_train_set = pd.read_csv(train_set_path)\n",
        "len(original_train_set)"
      ],
      "id": "paAsSPvgaeUF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lrsfcPtUxq_"
      },
      "outputs": [],
      "source": [
        "original_train_set = pd.read_csv(train_set_path)\n",
        "original_train_set = original_train_set[:100000]\n",
        "original_train_set.to_csv(save+'/sample_train.csv', index=False)"
      ],
      "id": "9lrsfcPtUxq_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77505e6e"
      },
      "outputs": [],
      "source": [
        "# Saving file names to variables\n",
        "trainloc = train_set_path\n",
        "#trainloc = os.path.join(DIR, 'dataset/sample_train.csv')\n",
        "valloc = save+'/validation_set.csv'\n",
        "testloc = save+'/test_set.csv'"
      ],
      "id": "77505e6e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e5d1b9b"
      },
      "outputs": [],
      "source": [
        "# Saving validation and test set to csv file\n",
        "validation_set.to_csv(valloc, index=False)\n",
        "test_set.to_csv(testloc, index=False)"
      ],
      "id": "9e5d1b9b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K8o3kRJhzhd",
        "outputId": "13968c28-0dd4-482c-b1bc-df8e35070470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torchtext                 0.3.1              \n"
          ]
        }
      ],
      "source": [
        "!pip list | grep torchtext"
      ],
      "id": "9K8o3kRJhzhd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ecbb0b5"
      },
      "outputs": [],
      "source": [
        "# Create Field object\n",
        "tokenize = lambda x: x.split()\n",
        "TEXT = data.Field(tokenize=tokenize, lower=False, include_lengths = True, init_token = '<SOS>', eos_token = '<EOS>')\n",
        "LEX = data.Field(tokenize=tokenize, lower=False, init_token = '<SOS>', eos_token = '<SOS>')\n",
        "BIO = data.Field(tokenize=tokenize, lower=False, init_token = '<SOS>', eos_token = '<SOS>')"
      ],
      "id": "4ecbb0b5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5136508"
      },
      "outputs": [],
      "source": [
        "# Specify Fields in the dataset\n",
        "fields = [('context', TEXT), ('question', TEXT), ('bio', BIO), ('lex', LEX)]"
      ],
      "id": "e5136508"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3666786c"
      },
      "outputs": [],
      "source": [
        "# Build the dataset\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(path = '',train=trainloc, validation=valloc,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   test=testloc, fields = fields, format='csv', skip_header=True)"
      ],
      "id": "3666786c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0fc2914"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary\n",
        "# MAX_VOCAB_SIZE = 50000\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "MIN_COUNT = 5\n",
        "BATCH_SIZE = batch_size"
      ],
      "id": "c0fc2914"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8918955f",
        "outputId": "be6ae5b4-296f-4032-aa25-a551aa41a631"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:38, 5.42MB/s]                                                \n",
            "100%|███████████████████████████████████████████████████▉| 399543/400000 [00:55<00:00, 6019.43it/s]"
          ]
        }
      ],
      "source": [
        "if word_vector == 'glove':\n",
        "\tTEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE,\n",
        "                 min_freq=MIN_COUNT, vectors='glove.6B.300d',\n",
        "                 unk_init=torch.Tensor.normal_)\n",
        "else:\n",
        "\tcache_ = numberbatch_loc\n",
        "\tvectors = Vectors(name='numberbatch-en-19.08.txt', cache=cache_)\n",
        "\tTEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE,\n",
        "                 min_freq=MIN_COUNT, vectors=vectors,\n",
        "                 unk_init=torch.Tensor.normal_)"
      ],
      "id": "8918955f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27e02b28"
      },
      "outputs": [],
      "source": [
        "BIO.build_vocab(train_data)\n",
        "LEX.build_vocab(train_data)\n",
        "\n",
        "# Create a set of iterators for each split\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x:len(x.context),\n",
        "     device = device)"
      ],
      "id": "27e02b28"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e14de328"
      },
      "outputs": [],
      "source": [
        "pad_idx = TEXT.vocab.stoi['<pad>']\n",
        "eos_idx = TEXT.vocab.stoi['<EOS>']\n",
        "sos_idx = TEXT.vocab.stoi['<SOS>']\n",
        "\n",
        "# Size of embedding_dim should match the dim of pre-trained word embeddings\n",
        "embedding_dim = 300\n",
        "hidden_dim = 512\n",
        "vocab_size = len(TEXT.vocab)"
      ],
      "id": "e14de328"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA6DGkMku6X7",
        "outputId": "16bfef80-a427-410a-f9b0-138c1f247db3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "download_squad.py  evaluate.py  models.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  README.md  \u001b[01;34mruns\u001b[0m/  squad_parse.py  train.py\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ],
      "id": "GA6DGkMku6X7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXUSOSGOIrfE"
      },
      "outputs": [],
      "source": [
        "# References: https://medium.com/@adam.wearne/seq2seq-with-pytorch-46dc00ff5164\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import random\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, embedding_size,\n",
        "                 embedding, answer_embedding, lexical_embedding, n_layers, dropout):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # Initialize network parameters\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Embedding layer to be shared with Decoder\n",
        "        self.embedding = embedding\n",
        "        self.answer_embedding = answer_embedding\n",
        "        self.lexical_embedding = lexical_embedding\n",
        "\n",
        "        # Bidirectional GRU\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size,\n",
        "                          num_layers=n_layers,\n",
        "                          dropout=dropout,\n",
        "                          bidirectional=True)\n",
        "\n",
        "    def forward(self, input_sequence, input_lengths, answer_sequence, lexical_sequence):\n",
        "\n",
        "        # Convert input_sequence to word embeddings\n",
        "        word_embeddings = self.embedding(input_sequence)\n",
        "        answer_embeddings = self.answer_embedding(answer_sequence)\n",
        "        lexical_embeddings = self.lexical_embedding(lexical_sequence)\n",
        "\n",
        "        # Concatenate word embeddings from all features\n",
        "        final_embeddings = torch.cat((word_embeddings,answer_embeddings,lexical_embeddings), 0)\n",
        "\n",
        "        # Pack the sequence of embeddings\n",
        "        packed_embeddings = nn.utils.rnn.pack_padded_sequence(final_embeddings, input_lengths)\n",
        "\n",
        "        # Run the packed embeddings through the GRU, and then unpack the sequences\n",
        "        outputs, hidden = self.gru(packed_embeddings)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "\n",
        "        # The ouput of a GRU has shape (seq_len, batch, hidden_size * num_directions)\n",
        "        # Because the Encoder is bidirectional, combine the results from the\n",
        "        # forward and reversed sequence by simply adding them together.\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "\n",
        "        return outputs, hidden\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def dot_score(self, hidden_state, encoder_states):\n",
        "        # Attention model use the dot product formula as global attention\n",
        "        return torch.sum(hidden_state * encoder_states, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        attn_scores = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_scores = attn_scores.t()\n",
        "\n",
        "        # Apply mask so network does not attend <pad> tokens\n",
        "        attn_scores = attn_scores.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        # Return softmax over attention scores\n",
        "        return F.softmax(attn_scores, dim=1).unsqueeze(1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding, embedding_size,\n",
        "                 hidden_size, output_size, n_layers, dropout):\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # Initialize network params\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.embedding = embedding\n",
        "\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size, n_layers,\n",
        "                          dropout=dropout)\n",
        "\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.attn = Attention(hidden_size)\n",
        "\n",
        "    def forward(self, current_token, hidden_state, encoder_outputs, mask):\n",
        "\n",
        "        # convert current_token to word_embedding\n",
        "        embedded = self.embedding(current_token)\n",
        "\n",
        "        # Pass through GRU\n",
        "        rnn_output, hidden_state = self.gru(embedded, hidden_state)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attention_weights = self.attn(rnn_output, encoder_outputs, mask)\n",
        "\n",
        "        # Calculate context vector\n",
        "        context = attention_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "\n",
        "        # Concatenate  context vector and GRU output\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "\n",
        "        # Pass concat_output to final output layer\n",
        "        output = self.out(concat_output)\n",
        "\n",
        "        # Return output and final hidden state\n",
        "        return output, hidden_state\n",
        "\n",
        "class Seq2seq(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, vocab_size,\n",
        "                 device, pad_idx, eos_idx, sos_idx, teacher_forcing_ratio=0.5):\n",
        "        super(Seq2seq, self).__init__()\n",
        "\n",
        "        # Initialize embedding layer shared by encoder and decoder\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.answer_embedding = nn.Embedding(6, embedding_size, padding_idx=1)\n",
        "        # Size could sometime change, depend on the device that the model is trained on\n",
        "        self.lexical_embedding = nn.Embedding(452, embedding_size, padding_idx=1)\n",
        "\n",
        "        # Encoder network\n",
        "        self.encoder = Encoder(hidden_size,\n",
        "                               embedding_size,\n",
        "                               self.embedding,\n",
        "                               self.answer_embedding,\n",
        "                               self.lexical_embedding,\n",
        "                               n_layers=2,\n",
        "                               dropout=0.5)\n",
        "\n",
        "        # Decoder network\n",
        "        self.decoder = Decoder(self.embedding,\n",
        "                               embedding_size,\n",
        "                               hidden_size,\n",
        "                               vocab_size,\n",
        "                               n_layers=2,\n",
        "                               dropout=0.5)\n",
        "\n",
        "\n",
        "        # Indices of special tokens and hardware device\n",
        "        self.pad_idx = pad_idx\n",
        "        self.eos_idx = eos_idx\n",
        "        self.sos_idx = sos_idx\n",
        "        self.device = device\n",
        "\n",
        "    def create_mask(self, input_sequence):\n",
        "\n",
        "        return (input_sequence != self.pad_idx).permute(1, 0)\n",
        "\n",
        "    def forward(self, input_sequence, answer_sequence, lexical_sequence, output_sequence, teacher_forcing_ratio):\n",
        "\n",
        "        # Unpack input_sequence tuple\n",
        "        input_tokens = input_sequence[0]\n",
        "        input_lengths = input_sequence[1]\n",
        "\n",
        "        # Unpack output_tokens, or create an empty tensor for text generation\n",
        "        if output_sequence is None:\n",
        "            inference = True\n",
        "            output_tokens = torch.zeros((100, input_tokens.shape[1])).long().fill_(self.sos_idx).to(self.device)\n",
        "        else:\n",
        "            inference = False\n",
        "            output_tokens = output_sequence[0]\n",
        "\n",
        "        vocab_size = self.decoder.output_size\n",
        "\n",
        "        batch_size = len(input_lengths)\n",
        "        max_seq_len = len(output_tokens)\n",
        "\n",
        "        # Tensor initialization to store Decoder output\n",
        "        outputs = torch.zeros(max_seq_len, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "        # Pass through the first half of the network\n",
        "        encoder_outputs, hidden = self.encoder(input_tokens, input_lengths, answer_sequence, lexical_sequence)\n",
        "\n",
        "        # Ensure dim of hidden_state can be fed into Decoder\n",
        "        hidden =  hidden[:self.decoder.n_layers]\n",
        "\n",
        "        # First input to the decoder is the <sos> tokens\n",
        "        output = output_tokens[0,:]\n",
        "\n",
        "        # Create mask\n",
        "        mask = self.create_mask(input_tokens)\n",
        "\n",
        "        # Step through the length of the output sequence one token at a time\n",
        "        # Teacher forcing is used to assist training\n",
        "        for t in range(1, max_seq_len):\n",
        "            output = output.unsqueeze(0)\n",
        "\n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs, mask)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (output_tokens[t] if teacher_force else top1)\n",
        "\n",
        "            # If we're in inference mode, keep generating until we produce an\n",
        "            # <eos> token\n",
        "            if inference and output.item() == self.eos_idx:\n",
        "                return outputs[:t]\n",
        "\n",
        "        return outputs"
      ],
      "id": "WXUSOSGOIrfE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1bcc81b",
        "outputId": "3ba138bd-ef10-472d-8a73-33a8b27564b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r100%|███████████████████████████████████████████████████▉| 399543/400000 [01:10<00:00, 6019.43it/s]"
          ]
        }
      ],
      "source": [
        "# Initializing weights\n",
        "# from models import Seq2seq\n",
        "model = Seq2seq(embedding_dim, hidden_dim, vocab_size, device, pad_idx, eos_idx, sos_idx).to(device)"
      ],
      "id": "a1bcc81b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ba191f6"
      },
      "outputs": [],
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors"
      ],
      "id": "3ba191f6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49aa149c",
        "outputId": "cda3c57d-43bd-421a-9973-f1b5e579e381"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-4.5283e-01,  6.7827e-01, -6.2774e-01,  ...,  2.0826e+00,\n",
              "          8.9462e-01,  1.0935e+00],\n",
              "        [-1.4745e+00,  6.2370e-01, -1.8066e+00,  ...,  4.8404e-02,\n",
              "          1.0767e+00, -1.6317e+00],\n",
              "        [ 6.6187e-01,  4.3202e-01, -1.6619e+00,  ...,  4.9365e-01,\n",
              "          3.8861e-01, -5.3126e-01],\n",
              "        ...,\n",
              "        [ 5.3634e-01,  1.9325e-03,  2.5228e-02,  ..., -6.6040e-02,\n",
              "         -2.3108e-01,  3.4183e-01],\n",
              "        [-1.7536e-01,  6.6148e-01, -6.2990e-01,  ..., -1.5426e+00,\n",
              "          6.9536e-01, -7.2572e-01],\n",
              "        [-1.4161e+00,  1.2042e-01,  6.2281e-01,  ..., -7.3627e-01,\n",
              "          1.0697e-01, -4.3506e-01]], device='cuda:0')"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "id": "49aa149c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6174531f"
      },
      "outputs": [],
      "source": [
        "# Initializing weights for special tokens\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)"
      ],
      "id": "6174531f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c0935a6"
      },
      "outputs": [],
      "source": [
        "model.embedding.weight.requires_grad = False\n",
        "\n",
        "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True],\n",
        "                       lr=1.0e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)"
      ],
      "id": "6c0935a6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK2fyjntGRsc",
        "outputId": "c6accaa2-1bb8-47be-c748-3c9506cf08ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/automatic-question-generation-master/dataset'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "save"
      ],
      "id": "aK2fyjntGRsc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57476eae"
      },
      "outputs": [],
      "source": [
        "# If continuing training\n",
        "# resume = os.path.join(save, 'model.pth')\n",
        "if (resume):\n",
        "\tmodel.load_state_dict(torch.load(resume))"
      ],
      "id": "57476eae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thnc-elzWf4p"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboard==1.14"
      ],
      "id": "thnc-elzWf4p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMio47wAWlcp"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall protobuf==3.7.0rc2"
      ],
      "id": "ZMio47wAWlcp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LECfuFkOTxnf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ],
      "id": "LECfuFkOTxnf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c125577"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, criterion, optimizer, clip):\n",
        "    # Put the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for idx, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
        "\n",
        "        input_sequence = batch.context\n",
        "        answer_sequence = batch.bio\n",
        "        output_sequence = batch.question\n",
        "        lexical_sequence = batch.lex\n",
        "\n",
        "        target_tokens = output_sequence[0]\n",
        "\n",
        "\n",
        "        # zero out the gradient for the current batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Run the batch through the model\n",
        "        output = model(input_sequence, answer_sequence, lexical_sequence, output_sequence, 0.5)\n",
        "\n",
        "        # Throw it through the loss function\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        target_tokens = target_tokens[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, target_tokens)\n",
        "        #writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "\n",
        "        # Perform back-prop and calculate the gradient of the loss function\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradient if necessary.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "id": "5c125577"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7c76ee0"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    # Put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for idx, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
        "\n",
        "            input_sequence = batch.context\n",
        "            answer_sequence = batch.bio\n",
        "            output_sequence = batch.question\n",
        "            lexical_sequence = batch.lex\n",
        "\n",
        "            target_tokens = output_sequence[0]\n",
        "\n",
        "            # Run the batch through the model\n",
        "            output = model(input_sequence, answer_sequence, lexical_sequence, output_sequence, 0)\n",
        "\n",
        "            # Throw it through the loss function\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            target_tokens = target_tokens[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, target_tokens)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "id": "f7c76ee0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83aa9fa6"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 50\n",
        "CLIP = 1\n"
      ],
      "id": "83aa9fa6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1cTqQZUOLFf",
        "outputId": "a3c283d4-9cce-4ad1-fc8e-07f4002b5e94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epochs"
      ],
      "id": "r1cTqQZUOLFf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7cf5329"
      },
      "outputs": [],
      "source": [
        "best_valid_loss = float('inf')"
      ],
      "id": "a7cf5329"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "a53a19e6",
        "outputId": "b1d73e16-64d6-4c0f-93a5-b26489c06683"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-76d1fc2f030b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#writer.flush()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'N_EPOCHS' is not defined"
          ]
        }
      ],
      "source": [
        "i=0\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    train_loss = train(model, train_iterator, criterion, optimizer, CLIP)\n",
        "    #writer.flush()\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    if i%10 ==0:\n",
        "      torch.save(model.state_dict(), save+f'/model_{i}.pth')\n",
        "    i+=1\n",
        "\n",
        "    # if valid_loss < best_valid_loss:\n",
        "        # best_valid_loss = valid_loss\n",
        "        # torch.save(model.state_dict(), save+'/model.pth')\n",
        "\n",
        "    print('Epoch: ', epoch)\n",
        "    print('Train loss: ', train_loss)\n",
        "    print('Valid loss: ', valid_loss)"
      ],
      "id": "a53a19e6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dd082d2"
      },
      "outputs": [],
      "source": [
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print('Test Loss: {:.2f}'.format(test_loss))\n",
        "\n",
        "for instance in list(tqdm._instances):\n",
        "    tqdm._decr_instances(instance)"
      ],
      "id": "6dd082d2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yWSeUZkedi-",
        "outputId": "6e3f3382-11eb-4a22-d939-4512b7e36109"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/automatic-question-generation-master/dataset/model.pth'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resume"
      ],
      "id": "3yWSeUZkedi-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P4LJnW-b7zQ"
      },
      "outputs": [],
      "source": [
        "!pip install -U spacy"
      ],
      "id": "0P4LJnW-b7zQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QpaVQ3whRmW"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "id": "-QpaVQ3whRmW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhsVg_r7cQpC",
        "outputId": "ed7e0302-bc1c-410f-ce8b-385e240a4332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "download_squad.py  evaluate.py  models.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  README.md  \u001b[01;34mruns\u001b[0m/  squad_parse.py  train.py\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ],
      "id": "bhsVg_r7cQpC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXIechBkcmU8",
        "outputId": "7abf9afa-5e85-4c26-fce5-4fdf3edd13b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(resume))"
      ],
      "id": "bXIechBkcmU8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kWB7qja1uKv"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ],
      "id": "_kWB7qja1uKv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFkbHoBOjNlh",
        "outputId": "67943344-c22c-4834-b0cd-43498221a498"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7db2a422bf10>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spacy.load('en_core_web_sm')"
      ],
      "id": "UFkbHoBOjNlh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RoJSPvxK2Rc"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "def translate_sentence(model, sentence, nlp):\n",
        "    model.eval()\n",
        "\n",
        "    tokenized = nlp(sentence)\n",
        "\n",
        "    tokenized = ['<sos>'] + [t.lower_ for t in tokenized] + ['<eos>']\n",
        "    numericalized = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "\n",
        "    sentence_length = torch.LongTensor([len(numericalized)]).to(model.device)\n",
        "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(model.device)\n",
        "\n",
        "    translation_tensor_logits = model((tensor, sentence_length), None, 0)\n",
        "\n",
        "    translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1)\n",
        "    translation = [TEXT.vocab.itos[t] for t in translation_tensor]\n",
        "\n",
        "    # Start at the first index.  We don't need to return the <sos> token...\n",
        "    translation = translation[1:]\n",
        "    return translation, translation_tensor_logits\n",
        "sentence = \"tell me a fun fact\"\n",
        "response, logits = translate_sentence(model, sentence, nlp)\n",
        "print(\" \".join(response))\n"
      ],
      "id": "_RoJSPvxK2Rc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "O4NGGudebRW2",
        "outputId": "4a3fefc7-62f4-4334-d43d-258febb4d960"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[44], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m translation, translation_tensor_logits\n\u001b[1;32m     32\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtell me a fun fact\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 33\u001b[0m response, logits \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(response))\n",
            "Cell \u001b[0;32mIn[44], line 23\u001b[0m, in \u001b[0;36mtranslate_sentence\u001b[0;34m(model, sentence, nlp)\u001b[0m\n\u001b[1;32m     20\u001b[0m output_sequence \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(tensor)  \u001b[38;5;66;03m# Use a tensor of zeros as a placeholder\u001b[39;00m\n\u001b[1;32m     21\u001b[0m teacher_forcing_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Replace with your desired teacher forcing ratio\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m translation_tensor_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m translation_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(translation_tensor_logits\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m translation \u001b[38;5;241m=\u001b[39m [TEXT\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mitos[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m translation_tensor]\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:550\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    552\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'output_sequence' and 'teacher_forcing_ratio'"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import torch\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Assuming you have a translation model defined and initialized as 'model'\n",
        "\n",
        "def translate_sentence(model, sentence, nlp):\n",
        "    model.eval()\n",
        "\n",
        "    tokenized = nlp(sentence)\n",
        "\n",
        "    tokenized = ['<sos>'] + [t.lower_ for t in tokenized] + ['<eos>']\n",
        "    numericalized = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "\n",
        "    sentence_length = torch.LongTensor([len(numericalized)]).to(model.device)\n",
        "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(model.device)\n",
        "\n",
        "    # You need to provide placeholders for 'output_sequence' and 'teacher_forcing_ratio'\n",
        "    output_sequence = torch.zeros_like(tensor)  # Use a tensor of zeros as a placeholder\n",
        "    teacher_forcing_ratio = 0.5  # Replace with your desired teacher forcing ratio\n",
        "\n",
        "    translation_tensor_logits = model((tensor, sentence_length), output_sequence, teacher_forcing_ratio)\n",
        "\n",
        "    translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1)\n",
        "    translation = [TEXT.vocab.itos[t] for t in translation_tensor]\n",
        "\n",
        "    # Start at the first index. We don't need to return the <sos> token...\n",
        "    translation = translation[1:]\n",
        "    return translation, translation_tensor_logits\n",
        "\n",
        "sentence = \"tell me a fun fact\"\n",
        "response, logits = translate_sentence(model, sentence, nlp)\n",
        "print(\" \".join(response))\n"
      ],
      "id": "O4NGGudebRW2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8JoqPk6jmbz"
      },
      "outputs": [],
      "source": [
        "# Code adapted from : https://github.com/bentrevett/pytorch-seq2seq\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import data\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "from tqdm import tqdm\n",
        "from models import Seq2seq\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "## Wordnet dependencies from meteor score\n",
        "#nltk.download('wordnet')\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load data\n",
        "trainloc = os.path.join(DIR, 'results/resultssquad_train.csv')\n",
        "valloc = os.path.join(DIR, 'dataset/validation_set.csv')\n",
        "testloc = os.path.join(DIR, 'dataset/test_set.csv')\n",
        "\n",
        "# Create Field object\n",
        "tokenize = lambda x: x.split()\n",
        "TEXT = data.Field(tokenize=tokenize, lower=False, include_lengths = True, init_token = '<SOS>', eos_token = '<EOS>')\n",
        "LEX = data.Field(tokenize=tokenize, lower=False, init_token = '<SOS>', eos_token = '<SOS>')\n",
        "BIO = data.Field(tokenize=tokenize, lower=False, init_token = '<SOS>', eos_token = '<SOS>')\n",
        "\n",
        "# Specify Fields in the dataset\n",
        "fields = [('context', TEXT), ('question', TEXT), ('bio', BIO), ('lex', LEX)]\n",
        "\n",
        "# Build the dataset\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(path = '',train=trainloc, validation=valloc,\n",
        "                                 test=testloc, fields = fields, format='csv', skip_header=True)\n",
        "\n",
        "# Build vocabulary\n",
        "MAX_VOCAB_SIZE = 50000\n",
        "MIN_COUNT = 5\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "if 'glove' == 'glove':\n",
        "  TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE,\n",
        "                 min_freq=MIN_COUNT, vectors='glove.6B.300d',\n",
        "                 unk_init=torch.Tensor.normal_)\n",
        "else:\n",
        "  cache_ = os.path.join(DIR, 'dataset')\n",
        "  vectors = Vectors(name='numberbatch-en-19.08.txt', cache=cache_)\n",
        "  TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE,\n",
        "                 min_freq=MIN_COUNT, vectors=vectors,\n",
        "                 unk_init=torch.Tensor.normal_)\n",
        "\n",
        "BIO.build_vocab(train_data)\n",
        "LEX.build_vocab(train_data)\n",
        "\n",
        "# Building model\n",
        "pad_idx = TEXT.vocab.stoi['<pad>']\n",
        "eos_idx = TEXT.vocab.stoi['<EOS>']\n",
        "sos_idx = TEXT.vocab.stoi['<SOS>']\n",
        "\n",
        "# Size of embedding_dim should match the dim of pre-trained word embeddings\n",
        "embedding_dim = 300\n",
        "hidden_dim = 512\n",
        "vocab_size = len(TEXT.vocab)\n",
        "\n",
        "# Initializing weights\n",
        "model = Seq2seq(embedding_dim, hidden_dim, vocab_size, device, pad_idx, eos_idx, sos_idx).to(device)\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "# Initializing weights for special tokens\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
        "\n",
        "model.embedding.weight.requires_grad = False\n",
        "\n",
        "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True],\n",
        "                       lr=1.0e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
        "\n",
        "# Load model\n",
        "model.load_state_dict(torch.load(resume))\n",
        "\n",
        "def predict_question(model, paragraph, answer_pos, lex_features):\n",
        "    model.eval()\n",
        "\n",
        "    tokenized = ['<SOS>'] + paragraph + ['<EOS>']\n",
        "    numericalized = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "\n",
        "    tokenized_answer = ['<SOS>'] + answer_pos + ['<EOS>']\n",
        "    numericalized_answer = [BIO.vocab.stoi[t] for t in tokenized_answer]\n",
        "\n",
        "    tokenized_lex = ['<SOS>'] + lex_features + ['<EOS>']\n",
        "    numericalized_lex = [LEX.vocab.stoi[t] for t in tokenized_lex]\n",
        "\n",
        "    paragraph_length = torch.LongTensor([len(numericalized)]).to(model.device)\n",
        "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(model.device)\n",
        "\n",
        "    answer_tensor = torch.LongTensor(numericalized_answer).unsqueeze(1).to(model.device)\n",
        "    lex_tensor = torch.LongTensor(numericalized_lex).unsqueeze(1).to(model.device)\n",
        "\n",
        "    question_tensor_logits = model((tensor, paragraph_length), answer_tensor, lex_tensor, None, 0)\n",
        "\n",
        "    question_tensor = torch.argmax(question_tensor_logits.squeeze(1), 1)\n",
        "    question = [TEXT.vocab.itos[t] for t in question_tensor]\n",
        "\n",
        "    # Start at the first index.  We don't need to return the <SOS> token\n",
        "    question = question[1:]\n",
        "\n",
        "    return question, question_tensor_logits\n",
        "\n",
        "# Display prediction\n",
        "num = 100\n",
        "example_idx = random.sample(range(1,300),num)\n",
        "\n",
        "for i in example_idx:\n",
        "  src = vars(train_data.examples[i])['context']\n",
        "  trg = vars(train_data.examples[i])['question']\n",
        "  ans = vars(train_data.examples[i])['bio']\n",
        "  lex = vars(train_data.examples[i])['lex']\n",
        "\n",
        "  print('context: ', ' '.join(src))\n",
        "  print('question: ', ' '.join(trg))\n",
        "\n",
        "  question, logits = predict_question(model, src, ans, lex)\n",
        "  print('predicted: ', \" \".join(question))\n",
        "  print()\n",
        "\n",
        "for j in example_idx:\n",
        "  src = vars(test_data.examples[j])['context']\n",
        "  trg = vars(test_data.examples[j])['question']\n",
        "  ans = vars(test_data.examples[j])['bio']\n",
        "  lex = vars(test_data.examples[j])['lex']\n",
        "\n",
        "  print('context: ', ' '.join(src))\n",
        "  print('question: ', ' '.join(trg))\n",
        "\n",
        "  question, logits = predict_question(model, src, ans, lex)\n",
        "  print('predicted: ', \" \".join(question))\n",
        "  print()\n",
        "\n"
      ],
      "id": "T8JoqPk6jmbz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c__YTaOZlU_7",
        "outputId": "f8447f53-86fa-475d-b86e-2664992c94e4"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m     meteor_score_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(meteor_score_)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bleu_score,meteor_score_\n\u001b[0;32m---> 30\u001b[0m bleu_score, meteor_score_ \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_bleu_and_meteor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLEU score = \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(bleu_score\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMETEOR score = \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(meteor_score_\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m))\n",
            "Cell \u001b[0;32mIn[14], line 23\u001b[0m, in \u001b[0;36mcalculate_bleu_and_meteor\u001b[0;34m(data, model)\u001b[0m\n\u001b[1;32m     21\u001b[0m     trgs\u001b[38;5;241m.\u001b[39mappend(trg)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# print(trg)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     meteor_score_\u001b[38;5;241m.\u001b[39mappend(\u001b[43msingle_meteor_score\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_trg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m corpus_bleu(pred_trgs, trgs)\n\u001b[1;32m     26\u001b[0m meteor_score_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(meteor_score_)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/nltk/translate/meteor_score.py:326\u001b[0m, in \u001b[0;36msingle_meteor_score\u001b[0;34m(reference, hypothesis, preprocess, stemmer, wordnet, alpha, beta, gamma)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingle_meteor_score\u001b[39m(\n\u001b[1;32m    283\u001b[0m     reference: Iterable[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    284\u001b[0m     hypothesis: Iterable[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m     gamma: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m    291\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m    Calculates METEOR score for single hypothesis and reference as per\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    \"Meteor: An Automatic Metric for MT Evaluation with HighLevels of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m    :return: The sentence-level METEOR score.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     enum_hypothesis, enum_reference \u001b[38;5;241m=\u001b[39m \u001b[43m_generate_enums\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     translation_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(enum_hypothesis)\n\u001b[1;32m    330\u001b[0m     reference_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(enum_reference)\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/nltk/translate/meteor_score.py:33\u001b[0m, in \u001b[0;36m_generate_enums\u001b[0;34m(hypothesis, reference, preprocess)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03mTakes in pre-tokenized inputs for hypothesis and reference and returns\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03menumerated word lists for each of them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m:return: enumerated words list\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hypothesis, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhypothesis\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expects pre-tokenized hypothesis (Iterable[str]): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhypothesis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(reference, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expects pre-tokenized reference (Iterable[str]): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreference\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m     )\n",
            "\u001b[0;31mTypeError\u001b[0m: \"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): where does the name rhine derive from ?"
          ]
        }
      ],
      "source": [
        "def calculate_bleu_and_meteor(data, model):\n",
        "\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    meteor_score_ = []\n",
        "\n",
        "    for datum in data:\n",
        "\n",
        "        src = vars(datum)['context']\n",
        "        trg = vars(datum)['question']\n",
        "        ans = vars(datum)['bio']\n",
        "        lex = vars(datum)['lex']\n",
        "\n",
        "        pred_trg, _ = predict_question(model, src, ans, lex)\n",
        "\n",
        "        #cut off <EOS> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "\n",
        "        pred_trgs.append(pred_trg)\n",
        "        # print(pred_trg)\n",
        "        trgs.append(trg)\n",
        "        # print(trg)\n",
        "        meteor_score_.append(single_meteor_score(' '.join(pred_trg),' '.join(trg)))\n",
        "\n",
        "    bleu_score = corpus_bleu(pred_trgs, trgs)\n",
        "    meteor_score_ = np.mean(meteor_score_)\n",
        "\n",
        "    return bleu_score,meteor_score_\n",
        "\n",
        "bleu_score, meteor_score_ = calculate_bleu_and_meteor(test_data, model)\n",
        "\n",
        "print('BLEU score = {:.2f}'.format(bleu_score*100))\n",
        "print('METEOR score = {:.2f}'.format(meteor_score_*100))"
      ],
      "id": "c__YTaOZlU_7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzc4IYlFowRH"
      },
      "outputs": [],
      "source": [],
      "id": "vzc4IYlFowRH"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py38",
      "name": "py38"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}