{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from en-core-web-md==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sunilthapa/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text, answer, answer_start, nlp):\n",
    "    '''\n",
    "    Extract answers to obtain POS, NER, case, BIO features based on text\n",
    "\n",
    "    Arguments:\n",
    "        text\t-- context or paragraph\n",
    "        answer \t-- answer in paragraph's question\n",
    "        answer_start -- starting index of answer\n",
    "        nlp \t-- spacy tool for nlp\n",
    "    Returns:\n",
    "        pos \t-- sequence of string of answer tokens part-of-speech tagging\n",
    "\t\tner \t-- sequence of string of answer tokens named entity recognition\n",
    "\t\tcase\t-- sequence of string of answer tokens case\n",
    "\t\tbio \t-- sequence of string of answer tokens inside-outside-beggining tagging\n",
    "\t\ttokenized \t-- joined tokenized context (paragraph) with lower typecasting\n",
    "    '''\n",
    "    \n",
    "    # Extract answer location index (left, right and answers itself) in text\n",
    "    left = text[0:answer_start]\n",
    "    ans = text[answer_start:answer_start+len(answer)+1]\n",
    "    right = text[answer_start+len(answer)+1:len(text)+1]    \n",
    "    \n",
    "    # Initialize return values list\n",
    "    pos = []\n",
    "    ner = []\n",
    "    case = []\n",
    "    bio = []\n",
    "    tokenized = []\n",
    "    \n",
    "    left_side = nlp(left)\n",
    "    answer_range = nlp(ans)\n",
    "    right_side = nlp(right)\n",
    "    \n",
    "    for token in left_side:\n",
    "        if token.text != '' and not token.text.isspace():\n",
    "            tokenized.append(token.text.lower())\n",
    "            pos.append(token.pos_)\n",
    "\n",
    "            if token.ent_type_ == '':\n",
    "                ner.append('O')\n",
    "            else:\n",
    "                ner.append(token.ent_type_)\n",
    "\n",
    "            if token.text[0].isupper():\n",
    "                case.append('UP')\n",
    "            else:\n",
    "                case.append('LOW')\n",
    "\n",
    "            bio.append('O')\n",
    "    \n",
    "    for token in answer_range:\n",
    "        if token.text != '' and not token.text.isspace():\n",
    "            tokenized.append(token.text.lower())\n",
    "            pos.append(token.pos_)\n",
    "\n",
    "            if token.ent_type_ == '':\n",
    "                ner.append('O')\n",
    "            else:\n",
    "                ner.append(token.ent_type_)\n",
    "\n",
    "            if token.text[0].isupper():\n",
    "                case.append('UP')\n",
    "            else:\n",
    "                case.append('LOW')\n",
    "\n",
    "            if token.i == 0:\n",
    "                bio.append('B')\n",
    "            else:\n",
    "                bio.append('I')\n",
    "    \n",
    "    for token in right_side:\n",
    "        if token.text != '' and not token.text.isspace():\n",
    "            tokenized.append(token.text.lower())\n",
    "            pos.append(token.pos_)\n",
    "\n",
    "            if token.ent_type_ == '':\n",
    "                ner.append('O')\n",
    "            else:\n",
    "                ner.append(token.ent_type_)\n",
    "\n",
    "            if token.text[0].isupper():\n",
    "                case.append('UP')\n",
    "            else:\n",
    "                case.append('LOW')\n",
    "\n",
    "            bio.append('O')\n",
    "                \n",
    "    return (' '.join(pos)), (' '.join(ner)), (' '.join(case)), (' '.join(bio)), (' '.join(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lexical_features(data):\n",
    "    '''\n",
    "    Creating pandas dataframe of features from parsed data\n",
    "\n",
    "    Arguments:\n",
    "        data -- data to be extracted; data must have context, answer, answer_start and question column\n",
    "    Returns:\n",
    "        data -- pandas dataframe of questions, context and features: IOB tag and lexical features(POS tag, NER, and case). \n",
    "    '''\n",
    "    data['BIO'] = ''\n",
    "    data['LEX'] = ''\n",
    "    count = 0\n",
    "    for idx, text, answer, answer_start, question in data[['context', 'answer', 'answer_start','question']].itertuples():\n",
    "        print(text)\n",
    "        pos, ner, case, data['BIO'][idx], data['context'][idx] = extract_features(text, str(answer), int(answer_start), nlp)\n",
    "        lex = [i + '_' + j + '_' + k for i, j, k in zip(pos.split(), ner.split(), case.split())]\n",
    "        data['LEX'][idx] = ' '.join(lex)\n",
    "        data['question'][idx] = ' '.join([token.text.lower() for token in nlp(question)])\n",
    "        count+=1\n",
    "        print(count)\n",
    "\n",
    "    # Building data on selected columns\n",
    "    data = data[['context', 'question', 'BIO', 'LEX']]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def build_lexical_features(data):\n",
    "#     '''\n",
    "#     Creating pandas dataframe of features from parsed data\n",
    "\n",
    "#     Arguments:\n",
    "#         data -- data to be extracted; data must have context, answer, answer_start, and question columns\n",
    "#         nlp -- spaCy language model\n",
    "#     Returns:\n",
    "#         data -- pandas dataframe of questions, context, and features: IOB tag and lexical features (POS tag, NER, and case). \n",
    "#     '''\n",
    "#     bio_tags = []\n",
    "#     lex_features = []\n",
    "\n",
    "#     for text, answer, answer_start, question in zip(data['context'], data['answer'], data['answer_start'], data['question']):\n",
    "#         pos, ner, case, bio_tag, context = extract_features(text, str(answer), int(answer_start), nlp)\n",
    "        \n",
    "#         lex = [f\"{i}_{j}_{k}\" for i, j, k in zip(pos.split(), ner.split(), case.split())]\n",
    "        \n",
    "#         bio_tags.append(bio_tag)\n",
    "#         lex_features.append(' '.join(lex))\n",
    "\n",
    "#     data['BIO'] = bio_tags\n",
    "#     data['LEX'] = lex_features\n",
    "\n",
    "#     # Processing the 'question' column\n",
    "#     data['question'] = data['question'].apply(lambda q: ' '.join([token.text.lower() for token in nlp(q)]))\n",
    "\n",
    "#     # Building data on selected columns\n",
    "#     data = data[['context', 'question', 'BIO', 'LEX']]\n",
    "\n",
    "#     return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "context = 'Hugging Face provides a platform called the Model Hub where you can upload and share models, including PyTorch models. Here is a general outline of the steps to upload a PyTorch model to the Hugging Face Model Hub'\n",
    "answer = 'Hugging Face Model Hub'\n",
    "answer_start = 0\n",
    "question = 'What is the name of the platform provided by Hugging Face?'\n",
    "question_end = len(question) - 1\n",
    "\n",
    "df = pd.DataFrame({'context':[context], 'question':[question], 'answer':[answer], 'answer_start':[answer_start]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face provides a platform called the Model Hub where you can upload and share models, including PyTorch models. Here is a general outline of the steps to upload a PyTorch model to the Hugging Face Model Hub\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4z/9ddz1dxn051gf97hvy69q3f80000gn/T/ipykernel_73516/2622661100.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pos, ner, case, data['BIO'][idx], data['context'][idx] = extract_features(text, str(answer), int(answer_start), nlp)\n",
      "/var/folders/4z/9ddz1dxn051gf97hvy69q3f80000gn/T/ipykernel_73516/2622661100.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pos, ner, case, data['BIO'][idx], data['context'][idx] = extract_features(text, str(answer), int(answer_start), nlp)\n",
      "/var/folders/4z/9ddz1dxn051gf97hvy69q3f80000gn/T/ipykernel_73516/2622661100.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['LEX'][idx] = ' '.join(lex)\n",
      "/var/folders/4z/9ddz1dxn051gf97hvy69q3f80000gn/T/ipykernel_73516/2622661100.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['question'][idx] = ' '.join([token.text.lower() for token in nlp(question)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>BIO</th>\n",
       "      <th>LEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hugging face provides a platform called the mo...</td>\n",
       "      <td>what is the name of the platform provided by h...</td>\n",
       "      <td>B I I I O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>VERB_O_UP PROPN_O_UP VERB_O_LOW PRON_O_LOW NOU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  hugging face provides a platform called the mo...   \n",
       "\n",
       "                                            question  \\\n",
       "0  what is the name of the platform provided by h...   \n",
       "\n",
       "                                                 BIO  \\\n",
       "0  B I I I O O O O O O O O O O O O O O O O O O O ...   \n",
       "\n",
       "                                                 LEX  \n",
       "0  VERB_O_UP PROPN_O_UP VERB_O_LOW PRON_O_LOW NOU...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = build_lexical_features(df)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References: https://medium.com/@adam.wearne/seq2seq-with-pytorch-46dc00ff5164\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import random\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, embedding_size,\n",
    "                 embedding, answer_embedding, lexical_embedding, n_layers, dropout):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Initialize network parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Embedding layer to be shared with Decoder\n",
    "        self.embedding = embedding\n",
    "        self.answer_embedding = answer_embedding\n",
    "        self.lexical_embedding = lexical_embedding\n",
    "\n",
    "        # Bidirectional GRU\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size,\n",
    "                          num_layers=n_layers,\n",
    "                          dropout=dropout,\n",
    "                          bidirectional=True)\n",
    "\n",
    "    def forward(self, input_sequence, input_lengths, answer_sequence, lexical_sequence):\n",
    "\n",
    "        # Convert input_sequence to word embeddings\n",
    "        word_embeddings = self.embedding(input_sequence)\n",
    "        answer_embeddings = self.answer_embedding(answer_sequence)\n",
    "        lexical_embeddings = self.lexical_embedding(lexical_sequence)\n",
    "\n",
    "        # Concatenate word embeddings from all features\n",
    "        final_embeddings = torch.cat((word_embeddings,answer_embeddings,lexical_embeddings), 0)\n",
    "\n",
    "        # Pack the sequence of embeddings\n",
    "        packed_embeddings = nn.utils.rnn.pack_padded_sequence(final_embeddings, input_lengths)\n",
    "\n",
    "        # Run the packed embeddings through the GRU, and then unpack the sequences\n",
    "        outputs, hidden = self.gru(packed_embeddings)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "\n",
    "        # The ouput of a GRU has shape (seq_len, batch, hidden_size * num_directions)\n",
    "        # Because the Encoder is bidirectional, combine the results from the\n",
    "        # forward and reversed sequence by simply adding them together.\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def dot_score(self, hidden_state, encoder_states):\n",
    "        # Attention model use the dot product formula as global attention\n",
    "        return torch.sum(hidden_state * encoder_states, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        attn_scores = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_scores = attn_scores.t()\n",
    "\n",
    "        # Apply mask so network does not attend <pad> tokens\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        # Return softmax over attention scores\n",
    "        return F.softmax(attn_scores, dim=1).unsqueeze(1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding, embedding_size,\n",
    "                 hidden_size, output_size, n_layers, dropout):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Initialize network params\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = embedding\n",
    "\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, n_layers,\n",
    "                          dropout=dropout)\n",
    "\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attention(hidden_size)\n",
    "\n",
    "    def forward(self, current_token, hidden_state, encoder_outputs, mask):\n",
    "\n",
    "        # convert current_token to word_embedding\n",
    "        embedded = self.embedding(current_token)\n",
    "\n",
    "        # Pass through GRU\n",
    "        rnn_output, hidden_state = self.gru(embedded, hidden_state)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.attn(rnn_output, encoder_outputs, mask)\n",
    "\n",
    "        # Calculate context vector\n",
    "        context = attention_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "\n",
    "        # Concatenate  context vector and GRU output\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Pass concat_output to final output layer\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden_state\n",
    "\n",
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size,\n",
    "                 device, pad_idx, eos_idx, sos_idx, teacher_forcing_ratio=0.5):\n",
    "        super(Seq2seq, self).__init__()\n",
    "\n",
    "        # Initialize embedding layer shared by encoder and decoder\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.answer_embedding = nn.Embedding(6, embedding_size, padding_idx=1)\n",
    "        # Size could sometime change, depend on the device that the model is trained on\n",
    "        self.lexical_embedding = nn.Embedding(452, embedding_size, padding_idx=1)\n",
    "\n",
    "        # Encoder network\n",
    "        self.encoder = Encoder(hidden_size,\n",
    "                               embedding_size,\n",
    "                               self.embedding,\n",
    "                               self.answer_embedding,\n",
    "                               self.lexical_embedding,\n",
    "                               n_layers=2,\n",
    "                               dropout=0.5)\n",
    "\n",
    "        # Decoder network\n",
    "        self.decoder = Decoder(self.embedding,\n",
    "                               embedding_size,\n",
    "                               hidden_size,\n",
    "                               vocab_size,\n",
    "                               n_layers=2,\n",
    "                               dropout=0.5)\n",
    "\n",
    "\n",
    "        # Indices of special tokens and hardware device\n",
    "        self.pad_idx = pad_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.device = device\n",
    "\n",
    "    def create_mask(self, input_sequence):\n",
    "\n",
    "        return (input_sequence != self.pad_idx).permute(1, 0)\n",
    "\n",
    "    def forward(self, input_sequence, answer_sequence, lexical_sequence, output_sequence, teacher_forcing_ratio):\n",
    "\n",
    "        # Unpack input_sequence tuple\n",
    "        input_tokens = input_sequence[0]\n",
    "        input_lengths = input_sequence[1]\n",
    "\n",
    "        # Unpack output_tokens, or create an empty tensor for text generation\n",
    "        if output_sequence is None:\n",
    "            inference = True\n",
    "            output_tokens = torch.zeros((100, input_tokens.shape[1])).long().fill_(self.sos_idx).to(self.device)\n",
    "        else:\n",
    "            inference = False\n",
    "            output_tokens = output_sequence[0]\n",
    "\n",
    "        vocab_size = self.decoder.output_size\n",
    "\n",
    "        batch_size = len(input_lengths)\n",
    "        max_seq_len = len(output_tokens)\n",
    "\n",
    "        # Tensor initialization to store Decoder output\n",
    "        outputs = torch.zeros(max_seq_len, batch_size, vocab_size).to(self.device)\n",
    "\n",
    "        # Pass through the first half of the network\n",
    "        encoder_outputs, hidden = self.encoder(input_tokens, input_lengths, answer_sequence, lexical_sequence)\n",
    "\n",
    "        # Ensure dim of hidden_state can be fed into Decoder\n",
    "        hidden =  hidden[:self.decoder.n_layers]\n",
    "\n",
    "        # First input to the decoder is the <sos> tokens\n",
    "        output = output_tokens[0,:]\n",
    "\n",
    "        # Create mask\n",
    "        mask = self.create_mask(input_tokens)\n",
    "\n",
    "        # Step through the length of the output sequence one token at a time\n",
    "        # Teacher forcing is used to assist training\n",
    "        for t in range(1, max_seq_len):\n",
    "            output = output.unsqueeze(0)\n",
    "\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs, mask)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (output_tokens[t] if teacher_force else top1)\n",
    "\n",
    "            # If we're in inference mode, keep generating until we produce an\n",
    "            # <eos> token\n",
    "            if inference and output.item() == self.eos_idx:\n",
    "                return outputs[:t]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>BIO</th>\n",
       "      <th>LEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hugging face provides a platform called the mo...</td>\n",
       "      <td>what is the name of the platform provided by h...</td>\n",
       "      <td>B I I I O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>VERB_O_UP PROPN_O_UP VERB_O_LOW PRON_O_LOW NOU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  hugging face provides a platform called the mo...   \n",
       "\n",
       "                                            question  \\\n",
       "0  what is the name of the platform provided by h...   \n",
       "\n",
       "                                                 BIO  \\\n",
       "0  B I I I O O O O O O O O O O O O O O O O O O O ...   \n",
       "\n",
       "                                                 LEX  \n",
       "0  VERB_O_UP PROPN_O_UP VERB_O_LOW PRON_O_LOW NOU...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'test.csv'\n",
    "data.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# build_vocab  data\n",
    "import torchtext\n",
    "from torchtext.data import Field, TabularDataset\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 1234\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load spacy models\n",
    "spacy_en = spacy.load('en_core_web_md')\n",
    "\n",
    "# Define fields\n",
    "tokenize = lambda x: x.split()\n",
    "TEXT = Field(tokenize=tokenize, lower=False, include_lengths = True, init_token = '<SOS>', eos_token = '<EOS>')\n",
    "LEX = Field(tokenize=tokenize, lower=False, init_token = '<SOS>', eos_token = '<SOS>')\n",
    "BIO = Field(tokenize=tokenize, lower=False, init_token = '<SOS>', eos_token = '<SOS>')\n",
    "\n",
    "\n",
    "# Load data\n",
    "fields = [('context', TEXT), ('question', TEXT), ('BIO', BIO), ('LEX', LEX)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TabularDataset(path=csv_path, format='csv', fields=fields, skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from : https://github.com/bentrevett/pytorch-seq2seq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "## Wordnet dependencies from meteor score\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load data\n",
    "trainloc = os.path.join(DIR, 'results/resultssquad_train.csv')\n",
    "valloc = os.path.join(DIR, 'dataset/validation_set.csv')\n",
    "testloc = os.path.join(DIR, 'dataset/test_set.csv')\n",
    "resume = os.path.join('models/model_14.pth')\n",
    "\n",
    "# Create Field object\n",
    "tokenize = lambda x: x.split()\n",
    "TEXT = data.Field(tokenize=tokenize, lower=False, include_lengths = True, init_token = '<SOS>', eos_token = '<EOS>')\n",
    "LEX = data.Field(tokenize=tokenize, lower=False, init_token = '<SOS>', eos_token = '<SOS>')\n",
    "BIO = data.Field(tokenize=tokenize, lower=False, init_token = '<SOS>', eos_token = '<SOS>')\n",
    "\n",
    "# Specify Fields in the dataset\n",
    "fields = [('context', TEXT), ('question', TEXT), ('bio', BIO), ('lex', LEX)]\n",
    "\n",
    "# Build the dataset\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(path = '',train=trainloc, validation=valloc,\n",
    "                                 test=testloc, fields = fields, format='csv', skip_header=True)\n",
    "\n",
    "# Build vocabulary\n",
    "MAX_VOCAB_SIZE = 35000\n",
    "MIN_COUNT = 5\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE,\n",
    "                min_freq=MIN_COUNT, vectors='glove.6B.300d',\n",
    "                unk_init=torch.Tensor.normal_)\n",
    "BIO.build_vocab(train_data)\n",
    "LEX.build_vocab(train_data)\n",
    "\n",
    "# Building model\n",
    "pad_idx = TEXT.vocab.stoi['<pad>']\n",
    "eos_idx = TEXT.vocab.stoi['<EOS>']\n",
    "sos_idx = TEXT.vocab.stoi['<SOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dir = os.path.join(DIR, 'vocabs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabularies\n",
    "torch.save(TEXT.vocab, os.path.join(vocab_dir, 'text_vocab.pth'))\n",
    "torch.save(LEX.vocab, os.path.join(vocab_dir, 'lex_vocab.pth'))\n",
    "torch.save(BIO.vocab, os.path.join(vocab_dir, 'bio_vocab.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabularies\n",
    "text_vocab = torch.load(os.path.join(vocab_dir, 'text_vocab.pth'))\n",
    "bio_vocab = torch.load(os.path.join(vocab_dir, 'bio_vocab.pth'))\n",
    "lex_vocab = torch.load(os.path.join(vocab_dir, 'lex_vocab.pth'))\n",
    "\n",
    "# Assign the loaded vocabularies to your fields\n",
    "TEXT.vocab = text_vocab\n",
    "BIO.vocab = bio_vocab\n",
    "LEX.vocab = lex_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35004 1 3 2\n"
     ]
    }
   ],
   "source": [
    "print(len(TEXT.vocab), pad_idx, eos_idx, sos_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of embedding_dim should match the dim of pre-trained word embeddings\n",
    "embedding_dim = 300\n",
    "hidden_dim = 512\n",
    "vocab_size = len(TEXT.vocab)\n",
    "\n",
    "# Initializing weights\n",
    "model = Seq2seq(embedding_dim, hidden_dim, vocab_size, device, pad_idx, eos_idx, sos_idx).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of embedding_dim should match the dim of pre-trained word embeddings\n",
    "embedding_dim = 300\n",
    "hidden_dim = 512\n",
    "vocab_size = 35004\n",
    "\n",
    "pad_idx = 1\n",
    "eos_idx = 3\n",
    "sos_idx = 2\n",
    "\n",
    "# Initializing weights\n",
    "model = Seq2seq(embedding_dim, hidden_dim, vocab_size, device, pad_idx, eos_idx, sos_idx).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model.load_state_dict(torch.load(resume, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = data.TabularDataset(path=csv_path, format='csv', fields=fields, skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context:  hugging face provides a platform called the model hub where you can upload and share models , including pytorch models . here is a general outline of the steps to upload a pytorch model to the hugging face model hub\n",
      "question:  what is the name of the platform provided by hugging face ?\n",
      "predicted:  what is a used to find a computer ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_question(model, paragraph, answer_pos, lex_features):\n",
    "    model.eval()\n",
    "\n",
    "    tokenized = ['<SOS>'] + paragraph + ['<EOS>']\n",
    "    numericalized = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "\n",
    "    tokenized_answer = ['<SOS>'] + answer_pos + ['<EOS>']\n",
    "    numericalized_answer = [BIO.vocab.stoi[t] for t in tokenized_answer]\n",
    "\n",
    "    tokenized_lex = ['<SOS>'] + lex_features + ['<EOS>']\n",
    "    numericalized_lex = [LEX.vocab.stoi[t] for t in tokenized_lex]\n",
    "\n",
    "    paragraph_length = torch.LongTensor([len(numericalized)]).to(model.device)\n",
    "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(model.device)\n",
    "\n",
    "    answer_tensor = torch.LongTensor(numericalized_answer).unsqueeze(1).to(model.device)\n",
    "    lex_tensor = torch.LongTensor(numericalized_lex).unsqueeze(1).to(model.device)\n",
    "\n",
    "    question_tensor_logits = model((tensor, paragraph_length), answer_tensor, lex_tensor, None, 0)\n",
    "\n",
    "    question_tensor = torch.argmax(question_tensor_logits.squeeze(1), 1)\n",
    "    question = [TEXT.vocab.itos[t] for t in question_tensor]\n",
    "\n",
    "    # Start at the first index.  We don't need to return the <SOS> token\n",
    "    question = question[1:]\n",
    "\n",
    "    return question, question_tensor_logits\n",
    "\n",
    "# Display prediction\n",
    "for example in test_dataset.examples:\n",
    "   src = vars(example)['context']\n",
    "   trg = vars(example)['question']\n",
    "   ans = vars(example)['bio']\n",
    "   lex = vars(example)['lex']\n",
    "\n",
    "   print('context: ', ' '.join(src))\n",
    "   print('question: ', ' '.join(trg))\n",
    "   question, logits = predict_question(model, src, ans, lex)\n",
    "   print('predicted: ', \" \".join(question))\n",
    "   print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
