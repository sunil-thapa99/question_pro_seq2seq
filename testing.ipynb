{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b2385a-0014-448c-a35c-c1ed506d5c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/site-packages (1.5.0)\n",
      "Requirement already satisfied: torchtext==0.3.1 in /usr/local/lib/python3.8/site-packages (0.3.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (from torch) (1.22.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.8/site-packages (from torch) (0.18.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from torchtext==0.3.1) (4.42.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from torchtext==0.3.1) (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from requests->torchtext==0.3.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->torchtext==0.3.1) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->torchtext==0.3.1) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->torchtext==0.3.1) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchtext==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff1ac13-9201-4dd6-93af-2f0fa4b1e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# References: https://medium.com/@adam.wearne/seq2seq-with-pytorch-46dc00ff5164\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import random\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, embedding_size,\n",
    "                 embedding, answer_embedding, lexical_embedding, n_layers, dropout):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Initialize network parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Embedding layer to be shared with Decoder\n",
    "        self.embedding = embedding\n",
    "        self.answer_embedding = answer_embedding\n",
    "        self.lexical_embedding = lexical_embedding\n",
    "\n",
    "        # Bidirectional GRU\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size,\n",
    "                          num_layers=n_layers,\n",
    "                          dropout=dropout,\n",
    "                          bidirectional=True)\n",
    "\n",
    "    def forward(self, input_sequence, input_lengths, answer_sequence, lexical_sequence):\n",
    "\n",
    "        # Convert input_sequence to word embeddings\n",
    "        word_embeddings = self.embedding(input_sequence)\n",
    "        answer_embeddings = self.answer_embedding(answer_sequence)\n",
    "        lexical_embeddings = self.lexical_embedding(lexical_sequence)\n",
    "\n",
    "        # Concatenate word embeddings from all features\n",
    "        final_embeddings = torch.cat((word_embeddings,answer_embeddings,lexical_embeddings), 0)\n",
    "\n",
    "        # Pack the sequence of embeddings\n",
    "        packed_embeddings = nn.utils.rnn.pack_padded_sequence(final_embeddings, input_lengths)\n",
    "\n",
    "        # Run the packed embeddings through the GRU, and then unpack the sequences\n",
    "        outputs, hidden = self.gru(packed_embeddings)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "\n",
    "        # The ouput of a GRU has shape (seq_len, batch, hidden_size * num_directions)\n",
    "        # Because the Encoder is bidirectional, combine the results from the\n",
    "        # forward and reversed sequence by simply adding them together.\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def dot_score(self, hidden_state, encoder_states):\n",
    "        # Attention model use the dot product formula as global attention\n",
    "        return torch.sum(hidden_state * encoder_states, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        attn_scores = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_scores = attn_scores.t()\n",
    "\n",
    "        # Apply mask so network does not attend <pad> tokens\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        # Return softmax over attention scores\n",
    "        return F.softmax(attn_scores, dim=1).unsqueeze(1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding, embedding_size,\n",
    "                 hidden_size, output_size, n_layers, dropout):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Initialize network params\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = embedding\n",
    "\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, n_layers,\n",
    "                          dropout=dropout)\n",
    "\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attention(hidden_size)\n",
    "\n",
    "    def forward(self, current_token, hidden_state, encoder_outputs, mask):\n",
    "\n",
    "        # convert current_token to word_embedding\n",
    "        embedded = self.embedding(current_token)\n",
    "\n",
    "        # Pass through GRU\n",
    "        rnn_output, hidden_state = self.gru(embedded, hidden_state)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.attn(rnn_output, encoder_outputs, mask)\n",
    "\n",
    "        # Calculate context vector\n",
    "        context = attention_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "\n",
    "        # Concatenate  context vector and GRU output\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Pass concat_output to final output layer\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden_state\n",
    "\n",
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size,\n",
    "                 device, pad_idx, eos_idx, sos_idx, teacher_forcing_ratio=0.5):\n",
    "        super(Seq2seq, self).__init__()\n",
    "\n",
    "        # Initialize embedding layer shared by encoder and decoder\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.answer_embedding = nn.Embedding(6, embedding_size, padding_idx=1)\n",
    "        # Size could sometime change, depend on the device that the model is trained on\n",
    "        self.lexical_embedding = nn.Embedding(452, embedding_size, padding_idx=1)\n",
    "\n",
    "        # Encoder network\n",
    "        self.encoder = Encoder(hidden_size,\n",
    "                               embedding_size,\n",
    "                               self.embedding,\n",
    "                               self.answer_embedding,\n",
    "                               self.lexical_embedding,\n",
    "                               n_layers=2,\n",
    "                               dropout=0.5)\n",
    "\n",
    "        # Decoder network\n",
    "        self.decoder = Decoder(self.embedding,\n",
    "                               embedding_size,\n",
    "                               hidden_size,\n",
    "                               vocab_size,\n",
    "                               n_layers=2,\n",
    "                               dropout=0.5)\n",
    "\n",
    "\n",
    "        # Indices of special tokens and hardware device\n",
    "        self.pad_idx = pad_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.device = device\n",
    "\n",
    "    def create_mask(self, input_sequence):\n",
    "\n",
    "        return (input_sequence != self.pad_idx).permute(1, 0)\n",
    "\n",
    "    def forward(self, input_sequence, answer_sequence, lexical_sequence, output_sequence, teacher_forcing_ratio):\n",
    "\n",
    "        # Unpack input_sequence tuple\n",
    "        input_tokens = input_sequence[0]\n",
    "        input_lengths = input_sequence[1]\n",
    "\n",
    "        # Unpack output_tokens, or create an empty tensor for text generation\n",
    "        if output_sequence is None:\n",
    "            inference = True\n",
    "            output_tokens = torch.zeros((100, input_tokens.shape[1])).long().fill_(self.sos_idx).to(self.device)\n",
    "        else:\n",
    "            inference = False\n",
    "            output_tokens = output_sequence[0]\n",
    "\n",
    "        vocab_size = self.decoder.output_size\n",
    "\n",
    "        batch_size = len(input_lengths)\n",
    "        max_seq_len = len(output_tokens)\n",
    "\n",
    "        # Tensor initialization to store Decoder output\n",
    "        outputs = torch.zeros(max_seq_len, batch_size, vocab_size).to(self.device)\n",
    "\n",
    "        # Pass through the first half of the network\n",
    "        encoder_outputs, hidden = self.encoder(input_tokens, input_lengths, answer_sequence, lexical_sequence)\n",
    "\n",
    "        # Ensure dim of hidden_state can be fed into Decoder\n",
    "        hidden =  hidden[:self.decoder.n_layers]\n",
    "\n",
    "        # First input to the decoder is the <sos> tokens\n",
    "        output = output_tokens[0,:]\n",
    "\n",
    "        # Create mask\n",
    "        mask = self.create_mask(input_tokens)\n",
    "\n",
    "        # Step through the length of the output sequence one token at a time\n",
    "        # Teacher forcing is used to assist training\n",
    "        for t in range(1, max_seq_len):\n",
    "            output = output.unsqueeze(0)\n",
    "\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs, mask)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (output_tokens[t] if teacher_force else top1)\n",
    "\n",
    "            # If we're in inference mode, keep generating until we produce an\n",
    "            # <eos> token\n",
    "            if inference and output.item() == self.eos_idx:\n",
    "                return outputs[:t]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c367a566-051f-4670-86fc-23a742e32778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/automatic-question-generation-master\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03b8c6b-2d18-4b46-a77a-d4d494ed6ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5548d9ed-192d-4f7d-b4ea-2a016b5f9c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks/automatic-question-generation-master'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d360a87-0fc4-4a56-8cde-48208dde8a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_path = os.path.join(DIR, 'results/resultssquad_train.csv')\n",
    "dev_set_path = os.path.join(DIR, 'results/resultssquad_dev.csv')\n",
    "test_size = 0.7\n",
    "save = os.path.join(DIR, 'dataset')\n",
    "train_set = os.path.join(DIR, 'dataset')\n",
    "word_vector = 'glove'\n",
    "batch_size = 128\n",
    "numberbatch_loc = os.path.join(DIR, 'dataset')\n",
    "resume = ''\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99155358-4313-4bbc-ab9c-9a59c18c021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving file names to variables\n",
    "train_set_path = os.path.join(DIR, 'results/resultssquad_train.csv')\n",
    "trainloc = train_set_path\n",
    "# trainloc = os.path.join(DIR, 'dataset/sample_train.csv')\n",
    "valloc = save+'/validation_set.csv'\n",
    "testloc = save+'/test_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93acbb6e-cc35-4fc3-ade2-d8b5230b27aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from : https://github.com/bentrevett/pytorch-seq2seq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "## Wordnet dependencies from meteor score\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load data\n",
    "trainloc = os.path.join(DIR, 'results/resultssquad_train.csv')\n",
    "valloc = os.path.join(DIR, 'dataset/validation_set.csv')\n",
    "testloc = os.path.join(DIR, 'dataset/test_set.csv')\n",
    "resume = os.path.join('models/model_14.pth')\n",
    "\n",
    "# Create Field object\n",
    "tokenize = lambda x: x.split()\n",
    "TEXT = data.Field(tokenize=tokenize, lower=False, include_lengths = True, init_token = '<SOS>', eos_token = '<EOS>')\n",
    "LEX = data.Field(tokenize=tokenize, lower=False, init_token = '<SOS>', eos_token = '<SOS>')\n",
    "BIO = data.Field(tokenize=tokenize, lower=False, init_token = '<SOS>', eos_token = '<SOS>')\n",
    "\n",
    "# Specify Fields in the dataset\n",
    "fields = [('context', TEXT), ('question', TEXT), ('bio', BIO), ('lex', LEX)]\n",
    "\n",
    "# Build the dataset\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(path = '',train=trainloc, validation=valloc,\n",
    "                                 test=testloc, fields = fields, format='csv', skip_header=True)\n",
    "\n",
    "# Build vocabulary\n",
    "MAX_VOCAB_SIZE = 35000\n",
    "MIN_COUNT = 5\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "if 'glove' == 'glove':\n",
    "  TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE,\n",
    "                 min_freq=MIN_COUNT, vectors='glove.6B.300d',\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "else:\n",
    "  cache_ = os.path.join(DIR, 'dataset')\n",
    "  vectors = Vectors(name='numberbatch-en-19.08.txt', cache=cache_)\n",
    "  TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE,\n",
    "                 min_freq=MIN_COUNT, vectors=vectors,\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "\n",
    "BIO.build_vocab(train_data)\n",
    "LEX.build_vocab(train_data)\n",
    "\n",
    "# Building model\n",
    "pad_idx = TEXT.vocab.stoi['<pad>']\n",
    "eos_idx = TEXT.vocab.stoi['<EOS>']\n",
    "sos_idx = TEXT.vocab.stoi['<SOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4258e88a-0167-4d99-bd86-4038bbf5e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of embedding_dim should match the dim of pre-trained word embeddings\n",
    "embedding_dim = 300\n",
    "hidden_dim = 512\n",
    "vocab_size = len(TEXT.vocab)\n",
    "\n",
    "# Initializing weights\n",
    "model = Seq2seq(embedding_dim, hidden_dim, vocab_size, device, pad_idx, eos_idx, sos_idx).to(device)\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# Initializing weights for special tokens\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
    "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
    "\n",
    "model.embedding.weight.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True],\n",
    "                       lr=1.0e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2d66f80-91b4-47e4-aa52-63007928c58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model.load_state_dict(torch.load(resume))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6194b0-e40d-496d-aece-7641e34bc9ca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict_question(model, paragraph, answer_pos, lex_features):\n",
    "    model.eval()\n",
    "\n",
    "    tokenized = ['<SOS>'] + paragraph + ['<EOS>']\n",
    "    numericalized = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "\n",
    "    tokenized_answer = ['<SOS>'] + answer_pos + ['<EOS>']\n",
    "    numericalized_answer = [BIO.vocab.stoi[t] for t in tokenized_answer]\n",
    "\n",
    "    tokenized_lex = ['<SOS>'] + lex_features + ['<EOS>']\n",
    "    numericalized_lex = [LEX.vocab.stoi[t] for t in tokenized_lex]\n",
    "\n",
    "    paragraph_length = torch.LongTensor([len(numericalized)]).to(model.device)\n",
    "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(model.device)\n",
    "\n",
    "    answer_tensor = torch.LongTensor(numericalized_answer).unsqueeze(1).to(model.device)\n",
    "    lex_tensor = torch.LongTensor(numericalized_lex).unsqueeze(1).to(model.device)\n",
    "\n",
    "    question_tensor_logits = model((tensor, paragraph_length), answer_tensor, lex_tensor, None, 0)\n",
    "\n",
    "    question_tensor = torch.argmax(question_tensor_logits.squeeze(1), 1)\n",
    "    question = [TEXT.vocab.itos[t] for t in question_tensor]\n",
    "\n",
    "    # Start at the first index.  We don't need to return the <SOS> token\n",
    "    question = question[1:]\n",
    "\n",
    "    return question, question_tensor_logits\n",
    "\n",
    "# Display prediction\n",
    "num = 100\n",
    "example_idx = random.sample(range(1,300),num)\n",
    "\n",
    "for i in example_idx:\n",
    "  src = vars(train_data.examples[i])['context']\n",
    "  trg = vars(train_data.examples[i])['question']\n",
    "  ans = vars(train_data.examples[i])['bio']\n",
    "  lex = vars(train_data.examples[i])['lex']\n",
    "\n",
    "  print('context: ', ' '.join(src))\n",
    "  print('question: ', ' '.join(trg))\n",
    "\n",
    "  question, logits = predict_question(model, src, ans, lex)\n",
    "  print('predicted: ', \" \".join(question))\n",
    "  print()\n",
    "\n",
    "for j in example_idx:\n",
    "  src = vars(test_data.examples[j])['context']\n",
    "  trg = vars(test_data.examples[j])['question']\n",
    "  ans = vars(test_data.examples[j])['bio']\n",
    "  lex = vars(test_data.examples[j])['lex']\n",
    "\n",
    "  print('context: ', ' '.join(src))\n",
    "  print('question: ', ' '.join(trg))\n",
    "\n",
    "  question, logits = predict_question(model, src, ans, lex)\n",
    "  print('predicted: ', \" \".join(question))\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90586540-0c06-4d91-b742-27663791a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_and_meteor(data, model):\n",
    "\n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    meteor_score_ = []\n",
    "\n",
    "    for datum in data:\n",
    "\n",
    "        src = vars(datum)['context']\n",
    "        trg = vars(datum)['question']\n",
    "        ans = vars(datum)['bio']\n",
    "        lex = vars(datum)['lex']\n",
    "\n",
    "        pred_trg, _ = predict_question(model, src, ans, lex)\n",
    "\n",
    "        #cut off <EOS> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "\n",
    "        pred_trgs.append(pred_trg)\n",
    "        # print(pred_trg)\n",
    "        trgs.append(trg)\n",
    "        # print(trg)\n",
    "        meteor_score_.append(single_meteor_score(' '.join(pred_trg),' '.join(trg)))\n",
    "\n",
    "    bleu_score = corpus_bleu(pred_trgs, trgs)\n",
    "    meteor_score_ = np.mean(meteor_score_)\n",
    "\n",
    "    return bleu_score,meteor_score_\n",
    "\n",
    "bleu_score, meteor_score_ = calculate_bleu_and_meteor(test_data, model)\n",
    "\n",
    "print('BLEU score = {:.2f}'.format(bleu_score*100))\n",
    "print('METEOR score = {:.2f}'.format(meteor_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9ffb398-c83f-4f64-8e79-1bd4c9bc8512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88d7b7a6-9253-4ea3-b315-8058f2d8d7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 1.46\n",
      "METEOR score = 16.17\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu_and_meteor(data, model):\n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    meteor_score_ = []\n",
    "\n",
    "    for datum in data:\n",
    "        src = vars(datum)['context']\n",
    "        trg = vars(datum)['question']\n",
    "        ans = vars(datum)['bio']\n",
    "        lex = vars(datum)['lex']\n",
    "\n",
    "        pred_trg, _ = predict_question(model, src, ans, lex)\n",
    "\n",
    "        # Cut off <EOS> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "\n",
    "        # Convert lists to strings if needed\n",
    "        trg = ' '.join(trg)\n",
    "        pred_trg = ' '.join(pred_trg)\n",
    "\n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append(trg)\n",
    "\n",
    "        meteor_score_.append(single_meteor_score(pred_trg.split(), trg.split()))\n",
    "\n",
    "    bleu_score = corpus_bleu([[trg.split()] for trg in trgs], [pred_trg.split() for pred_trg in pred_trgs])\n",
    "    meteor_score_ = np.mean(meteor_score_)\n",
    "\n",
    "    return bleu_score, meteor_score_\n",
    "\n",
    "# Calculate BLEU and METEOR scores\n",
    "bleu_score, meteor_score_ = calculate_bleu_and_meteor(test_data, model)\n",
    "\n",
    "print('BLEU score = {:.2f}'.format(bleu_score * 100))\n",
    "print('METEOR score = {:.2f}'.format(meteor_score_ * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f9818-5591-4a4c-ac84-456832c35149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
