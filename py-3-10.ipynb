{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: torchvision in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: torchaudio in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: numpy in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.1.0\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /Users/basisushil/opt/anaconda3/lib/python3.11/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: torchaudio, torchtext, torchvision\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                         2.1.0\n",
      "torchaudio                    2.1.0\n",
      "torchtext                     0.3.1\n",
      "torchvision                   0.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/basisushil/Desktop/automatic-question-generation-master\n",
      "/Users/basisushil/Desktop/automatic-question-generation-master/main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "DIR = os.getcwd()\n",
    "main_dir = os.path.join(os.getcwd(), 'main')\n",
    "print(main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from main.models import Seq2seq\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/basisushil/Desktop/automatic-question-generation-master/result_squad/resultssquad_train.csv /Users/basisushil/Desktop/automatic-question-generation-master/result_squad/resultssquad_dev.csv /Users/basisushil/Desktop/automatic-question-generation-master/dataset/\n"
     ]
    }
   ],
   "source": [
    "train_set = os.path.join(DIR, 'result_squad/resultssquad_train.csv')\n",
    "dev_set = os.path.join(DIR, 'result_squad/resultssquad_dev.csv')\n",
    "test_size = 0.7\n",
    "save = os.path.join(DIR, 'dataset/')\n",
    "word_vector = 'glove'\n",
    "batch_size = 128\n",
    "resume = ''\n",
    "epochs = 100\n",
    "numberbatch_loc  = os.path.join(DIR, 'dataset/')\n",
    "\n",
    "print(train_set, dev_set, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dev dataset into test set and validation set\n",
    "dev_set = pd.read_csv(dev_set)\n",
    "validation_set, test_set = train_test_split(dev_set, test_size = test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving file names to variables\n",
    "trainloc = train_set\n",
    "valloc = save+'validation_set.csv'\n",
    "testloc = save+'test_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/basisushil/Desktop/automatic-question-generation-master/dataset/validation_set.csv'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving validation and test set to csv file\n",
    "validation_set.to_csv(valloc, index=False)\n",
    "test_set.to_csv(testloc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Field object\n",
    "tokenize = lambda x: x.split()\n",
    "TEXT = data.Field(tokenize=tokenize, lower=False, include_lengths = True, init_token = '<SOS>', eos_token = '<EOS>')\n",
    "LEX = data.Field(tokenize=tokenize, lower=False, init_token = '<SOS>', eos_token = '<SOS>')\n",
    "BIO = data.Field(tokenize=tokenize, lower=False, init_token = '<SOS>', eos_token = '<SOS>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Fields in the dataset\n",
    "fields = [('context', TEXT), ('question', TEXT), ('bio', BIO), ('lex', LEX)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(path = '',train=trainloc, validation=valloc,\n",
    "                                                               test=testloc, fields = fields, format='csv', skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "MIN_COUNT = 5\n",
    "BATCH_SIZE = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word_vector == 'glove':\n",
    "\tTEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE,\n",
    "                 min_freq=MIN_COUNT, vectors='glove.6B.300d',\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "else:\n",
    "\tcache_ = numberbatch_loc \n",
    "\tvectors = Vectors(name='numberbatch-en-19.08.txt', cache=cache_)\n",
    "\tTEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE,\n",
    "                 min_freq=MIN_COUNT, vectors=vectors,\n",
    "                 unk_init=torch.Tensor.normal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIO.build_vocab(train_data)\n",
    "LEX.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of iterators for each split\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     sort_within_batch = True,\n",
    "     sort_key = lambda x:len(x.context),\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = TEXT.vocab.stoi['<pad>']\n",
    "eos_idx = TEXT.vocab.stoi['<EOS>']\n",
    "sos_idx = TEXT.vocab.stoi['<SOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of embedding_dim should match the dim of pre-trained word embeddings\n",
    "embedding_dim = 300\n",
    "hidden_dim = 512\n",
    "vocab_size = len(TEXT.vocab)\n",
    "# optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True], \n",
    "#                        lr=1.0e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing weights\n",
    "model = Seq2seq(embedding_dim, hidden_dim, vocab_size, device, pad_idx, eos_idx, sos_idx).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0391, -0.5870, -1.5240,  ..., -0.9038,  0.2345,  0.4160],\n",
       "        [-0.7859, -0.3455,  0.1419,  ..., -0.2658, -1.1860, -0.5025],\n",
       "        [-1.0757,  2.1462,  1.9535,  ..., -0.6773, -0.2611, -1.7673],\n",
       "        ...,\n",
       "        [-0.5509, -0.8127, -0.9347,  ...,  0.3362,  0.4070,  0.6546],\n",
       "        [-0.5043, -0.1691,  0.2737,  ..., -0.2468,  0.7967, -0.3363],\n",
       "        [-0.0176, -0.0626, -0.2060,  ...,  0.4832,  1.1497, -0.2633]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# Initializing weights for special tokens\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
    "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True], \n",
    "                       lr=1.0e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If continuing training\n",
    "if (resume):\n",
    "\tmodel.load_state_dict(torch.load(resume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion, optimizer, clip):\n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "        \n",
    "        input_sequence = batch.context\n",
    "        answer_sequence = batch.bio\n",
    "        output_sequence = batch.question\n",
    "        lexical_sequence = batch.lex\n",
    "        \n",
    "        target_tokens = output_sequence[0]\n",
    "        \n",
    "        # zero out the gradient for the current batch\n",
    "        optimizer.zero_grad()\n",
    "        print('----------------------', model, type(model))\n",
    "\n",
    "        # Run the batch through the model\n",
    "        output = model(input_sequence, answer_sequence, lexical_sequence, output_sequence, 0.5)\n",
    "\n",
    "        # Throw it through the loss function\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        target_tokens = target_tokens[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, target_tokens)\n",
    "        \n",
    "        # Perform back-prop and calculate the gradient of the loss function\n",
    "        loss.backward()\n",
    "          \n",
    "        # Clip the gradient if necessary.          \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for idx, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "\n",
    "            input_sequence = batch.context\n",
    "            answer_sequence = batch.bio\n",
    "            output_sequence = batch.question\n",
    "            lexical_sequence = batch.lex\n",
    "            \n",
    "            target_tokens = output_sequence[0]\n",
    "            \n",
    "            # Run the batch through the model\n",
    "            output = model(input_sequence, answer_sequence, lexical_sequence, output_sequence, 0)\n",
    "            \n",
    "            # Throw it through the loss function\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            target_tokens = target_tokens[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, target_tokens)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = epochs\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1019 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- Seq2seq(\n",
      "  (embedding): Embedding(50004, 300)\n",
      "  (answer_embedding): Embedding(6, 300, padding_idx=1)\n",
      "  (lexical_embedding): Embedding(452, 300, padding_idx=1)\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(50004, 300)\n",
      "    (answer_embedding): Embedding(6, 300, padding_idx=1)\n",
      "    (lexical_embedding): Embedding(452, 300, padding_idx=1)\n",
      "    (gru): GRU(300, 512, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(50004, 300)\n",
      "    (gru): GRU(300, 512, num_layers=2, dropout=0.5)\n",
      "    (concat): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (out): Linear(in_features=512, out_features=50004, bias=True)\n",
      "    (attn): Attention()\n",
      "  )\n",
      ") <class 'main.models.Seq2seq'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "'lengths' argument should be a 1D CPU int64 tensor, but got 1D mps:0 Long tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss \u001b[39m=\u001b[39m train(model, train_iterator, criterion, optimizer, CLIP)\n",
      "Cell \u001b[0;32mIn[114], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, criterion, optimizer, clip)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m----------------------\u001b[39m\u001b[39m'\u001b[39m, model, \u001b[39mtype\u001b[39m(model))\n\u001b[1;32m     19\u001b[0m \u001b[39m# Run the batch through the model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m output \u001b[39m=\u001b[39m model(input_sequence, answer_sequence, lexical_sequence, output_sequence, \u001b[39m0.5\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[39m# Throw it through the loss function\u001b[39;00m\n\u001b[1;32m     23\u001b[0m output \u001b[39m=\u001b[39m output[\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/automatic-question-generation-master/main/models.py:187\u001b[0m, in \u001b[0;36mSeq2seq.forward\u001b[0;34m(self, input_sequence, answer_sequence, lexical_sequence, output_sequence, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    184\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(max_seq_len, batch_size, vocab_size)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    186\u001b[0m \u001b[39m# Pass through the first half of the network\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m encoder_outputs, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(input_tokens, input_lengths, answer_sequence, lexical_sequence)\n\u001b[1;32m    189\u001b[0m \u001b[39m# Ensure dim of hidden_state can be fed into Decoder\u001b[39;00m\n\u001b[1;32m    190\u001b[0m hidden \u001b[39m=\u001b[39m  hidden[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mn_layers]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/automatic-question-generation-master/main/models.py:44\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, input_sequence, input_lengths, answer_sequence, lexical_sequence)\u001b[0m\n\u001b[1;32m     41\u001b[0m final_embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((word_embeddings,answer_embeddings,lexical_embeddings), \u001b[39m0\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[39m# Pack the sequence of embeddings\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m packed_embeddings \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpack_padded_sequence(final_embeddings, input_lengths\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     46\u001b[0m \u001b[39m# Run the packed embeddings through the GRU, and then unpack the sequences\u001b[39;00m\n\u001b[1;32m     47\u001b[0m outputs, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgru(packed_embeddings)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/utils/rnn.py:264\u001b[0m, in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    260\u001b[0m     batch_dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m batch_first \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    261\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mindex_select(batch_dim, sorted_indices)\n\u001b[1;32m    263\u001b[0m data, batch_sizes \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 264\u001b[0m     _VF\u001b[39m.\u001b[39m_pack_padded_sequence(\u001b[39minput\u001b[39m, lengths, batch_first)\n\u001b[1;32m    265\u001b[0m \u001b[39mreturn\u001b[39;00m _packed_sequence_init(data, batch_sizes, sorted_indices, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D mps:0 Long tensor"
     ]
    }
   ],
   "source": [
    "train_loss = train(model, train_iterator, criterion, optimizer, CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1019 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "'lengths' argument should be a 1D CPU int64 tensor, but got 1D mps:0 Long tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m         train_loss \u001b[39m=\u001b[39m train(model, train_iterator, criterion, optimizer, CLIP)\n\u001b[1;32m      5\u001b[0m         valid_loss \u001b[39m=\u001b[39m evaluate(model, valid_iterator, criterion)\n\u001b[1;32m      7\u001b[0m         \u001b[39mif\u001b[39;00m valid_loss \u001b[39m<\u001b[39m best_valid_loss:\n",
      "Cell \u001b[0;32mIn[54], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, criterion, optimizer, clip)\u001b[0m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m \u001b[39m# Run the batch through the model\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m output \u001b[39m=\u001b[39m model(input_sequence, answer_sequence, lexical_sequence, output_sequence, \u001b[39m0.5\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[39m# Throw it through the loss function\u001b[39;00m\n\u001b[1;32m     22\u001b[0m output \u001b[39m=\u001b[39m output[\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/automatic-question-generation-master/main/models.py:187\u001b[0m, in \u001b[0;36mSeq2seq.forward\u001b[0;34m(self, input_sequence, answer_sequence, lexical_sequence, output_sequence, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    184\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(max_seq_len, batch_size, vocab_size)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    186\u001b[0m \u001b[39m# Pass through the first half of the network\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m encoder_outputs, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(input_tokens, input_lengths, answer_sequence, lexical_sequence)\n\u001b[1;32m    189\u001b[0m \u001b[39m# Ensure dim of hidden_state can be fed into Decoder\u001b[39;00m\n\u001b[1;32m    190\u001b[0m hidden \u001b[39m=\u001b[39m  hidden[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mn_layers]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/automatic-question-generation-master/main/models.py:44\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, input_sequence, input_lengths, answer_sequence, lexical_sequence)\u001b[0m\n\u001b[1;32m     41\u001b[0m final_embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((word_embeddings,answer_embeddings,lexical_embeddings), \u001b[39m0\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[39m# Pack the sequence of embeddings\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m packed_embeddings \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpack_padded_sequence(final_embeddings, input_lengths\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     46\u001b[0m \u001b[39m# Run the packed embeddings through the GRU, and then unpack the sequences\u001b[39;00m\n\u001b[1;32m     47\u001b[0m outputs, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgru(packed_embeddings)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/utils/rnn.py:264\u001b[0m, in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    260\u001b[0m     batch_dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m batch_first \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    261\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mindex_select(batch_dim, sorted_indices)\n\u001b[1;32m    263\u001b[0m data, batch_sizes \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 264\u001b[0m     _VF\u001b[39m.\u001b[39m_pack_padded_sequence(\u001b[39minput\u001b[39m, lengths, batch_first)\n\u001b[1;32m    265\u001b[0m \u001b[39mreturn\u001b[39;00m _packed_sequence_init(data, batch_sizes, sorted_indices, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D mps:0 Long tensor"
     ]
    }
   ],
   "source": [
    "# for epoch in range(N_EPOCHS):\n",
    "for epoch in range(1):\n",
    "    try:\n",
    "        train_loss = train(model, train_iterator, criterion, optimizer, CLIP)\n",
    "        valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), save+'/model.pth')\n",
    "\n",
    "        print('Epoch: ', epoch)\n",
    "        print('Train loss: ', train_loss)\n",
    "        print('Valid loss: ', valid_loss)\n",
    "    except:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
